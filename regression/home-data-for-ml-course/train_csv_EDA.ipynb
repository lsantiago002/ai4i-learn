{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92bcab5c",
   "metadata": {},
   "source": [
    "**0. Loading and High-level Overview of the Dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7939d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from dython.nominal import associations\n",
    "import warnings \n",
    "\n",
    "# silence warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plotting settings\n",
    "plt.style.use(['seaborn-paper'])\n",
    "plt.rcParams['font.family'] = 'helvetica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279bfa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3bfefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "hse_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a270576",
   "metadata": {},
   "source": [
    "The number of rows and columns:\n",
    "    rows: 1460\n",
    "    columns: 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2246826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of data\n",
    "hse_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7407e491",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     43\n",
      "int64      35\n",
      "float64     3\n",
      "dtype: int64\n",
      "The number of categorical columns is 43.\n",
      "The number of numercial columns is 38.\n"
     ]
    }
   ],
   "source": [
    "# count data types\n",
    "print(hse_data.dtypes.value_counts())\n",
    "\n",
    "\n",
    "# another way\n",
    "cat_cols = hse_data.dtypes[hse_data.dtypes == 'object']\n",
    "num_cols = hse_data.dtypes[hse_data.dtypes != 'object']\n",
    "print(\"The number of categorical columns is {}.\".format(len(cat_cols)))\n",
    "print(\"The number of numercial columns is {}.\".format(len(num_cols)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deafdd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the column names \n",
    "hse_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dacda3",
   "metadata": {},
   "source": [
    "By understanding the number of features of either numerical/categorical types, \n",
    "we are able to better plan out the necessary visualisation required to best present each columns. \n",
    "You will see this subsequently later in Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d60fe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Id   MSSubClass MSZoning  LotFrontage        LotArea Street  \\\n",
      "count   1460.000000  1460.000000     1460  1201.000000    1460.000000   1460   \n",
      "unique          NaN          NaN        5          NaN            NaN      2   \n",
      "top             NaN          NaN       RL          NaN            NaN   Pave   \n",
      "freq            NaN          NaN     1151          NaN            NaN   1454   \n",
      "mean     730.500000    56.897260      NaN    70.049958   10516.828082    NaN   \n",
      "std      421.610009    42.300571      NaN    24.284752    9981.264932    NaN   \n",
      "min        1.000000    20.000000      NaN    21.000000    1300.000000    NaN   \n",
      "25%      365.750000    20.000000      NaN    59.000000    7553.500000    NaN   \n",
      "50%      730.500000    50.000000      NaN    69.000000    9478.500000    NaN   \n",
      "75%     1095.250000    70.000000      NaN    80.000000   11601.500000    NaN   \n",
      "max     1460.000000   190.000000      NaN   313.000000  215245.000000    NaN   \n",
      "\n",
      "       Alley LotShape LandContour Utilities  ...     PoolArea PoolQC  Fence  \\\n",
      "count     91     1460        1460      1460  ...  1460.000000      7    281   \n",
      "unique     2        4           4         2  ...          NaN      3      4   \n",
      "top     Grvl      Reg         Lvl    AllPub  ...          NaN     Gd  MnPrv   \n",
      "freq      50      925        1311      1459  ...          NaN      3    157   \n",
      "mean     NaN      NaN         NaN       NaN  ...     2.758904    NaN    NaN   \n",
      "std      NaN      NaN         NaN       NaN  ...    40.177307    NaN    NaN   \n",
      "min      NaN      NaN         NaN       NaN  ...     0.000000    NaN    NaN   \n",
      "25%      NaN      NaN         NaN       NaN  ...     0.000000    NaN    NaN   \n",
      "50%      NaN      NaN         NaN       NaN  ...     0.000000    NaN    NaN   \n",
      "75%      NaN      NaN         NaN       NaN  ...     0.000000    NaN    NaN   \n",
      "max      NaN      NaN         NaN       NaN  ...   738.000000    NaN    NaN   \n",
      "\n",
      "       MiscFeature       MiscVal       MoSold       YrSold  SaleType  \\\n",
      "count           54   1460.000000  1460.000000  1460.000000      1460   \n",
      "unique           4           NaN          NaN          NaN         9   \n",
      "top           Shed           NaN          NaN          NaN        WD   \n",
      "freq            49           NaN          NaN          NaN      1267   \n",
      "mean           NaN     43.489041     6.321918  2007.815753       NaN   \n",
      "std            NaN    496.123024     2.703626     1.328095       NaN   \n",
      "min            NaN      0.000000     1.000000  2006.000000       NaN   \n",
      "25%            NaN      0.000000     5.000000  2007.000000       NaN   \n",
      "50%            NaN      0.000000     6.000000  2008.000000       NaN   \n",
      "75%            NaN      0.000000     8.000000  2009.000000       NaN   \n",
      "max            NaN  15500.000000    12.000000  2010.000000       NaN   \n",
      "\n",
      "        SaleCondition      SalePrice  \n",
      "count            1460    1460.000000  \n",
      "unique              6            NaN  \n",
      "top            Normal            NaN  \n",
      "freq             1198            NaN  \n",
      "mean              NaN  180921.195890  \n",
      "std               NaN   79442.502883  \n",
      "min               NaN   34900.000000  \n",
      "25%               NaN  129975.000000  \n",
      "50%               NaN  163000.000000  \n",
      "75%               NaN  214000.000000  \n",
      "max               NaN  755000.000000  \n",
      "\n",
      "[11 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "# check summary statistic for columns\n",
    "print(hse_data.describe(include='all')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d6d14c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      1460.000000\n",
       "mean     180921.195890\n",
       "std       79442.502883\n",
       "min       34900.000000\n",
       "25%      129975.000000\n",
       "50%      163000.000000\n",
       "75%      214000.000000\n",
       "max      755000.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics of SalePrice column\n",
    "hse_data['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d744d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual',\n",
      "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
      "       'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
      "       'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence',\n",
      "       'MiscFeature'],\n",
      "      dtype='object')\n",
      "\n",
      "The total % of columns with missing values: 23.46 %\n"
     ]
    }
   ],
   "source": [
    "# (Jensen) check the prevalence of missing values\n",
    "# df.columns[df.isnull().sum() > 0]      # filtering for columns with null values\n",
    "\n",
    "# what is the percentage of missing columns in the bigger picture?\n",
    "missing_cols = hse_data.columns[hse_data.isnull().sum() > 0]\n",
    "print(missing_cols)\n",
    "print(f\"\\nThe total % of columns with missing values: {len(missing_cols)/hse_data.shape[1] * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b7943",
   "metadata": {},
   "source": [
    "(Jensen) We generated an overview statistics on the % of columns plagued with missing values. \n",
    "However, in order to better understand your data – let's look at it in a more in-depth manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54724c8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoolQC          99.520548\n",
      "MiscFeature     96.301370\n",
      "Alley           93.767123\n",
      "Fence           80.753425\n",
      "FireplaceQu     47.260274\n",
      "LotFrontage     17.739726\n",
      "GarageType       5.547945\n",
      "GarageYrBlt      5.547945\n",
      "GarageFinish     5.547945\n",
      "GarageQual       5.547945\n",
      "GarageCond       5.547945\n",
      "BsmtExposure     2.602740\n",
      "BsmtFinType2     2.602740\n",
      "BsmtFinType1     2.534247\n",
      "BsmtCond         2.534247\n",
      "BsmtQual         2.534247\n",
      "MasVnrArea       0.547945\n",
      "MasVnrType       0.547945\n",
      "Electrical       0.068493\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# missing percentage per columns or features over the overall data, multiplied by 100%\n",
    "missing_percentage = hse_data.isnull().sum() / len(hse_data) * 100\n",
    "# print(missing_percentage)\n",
    "\n",
    "# print only those with missing values percentage > 0\n",
    "print(missing_percentage[missing_percentage > 0].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee2931",
   "metadata": {},
   "source": [
    "(Jensen)\n",
    " Now, this missing_pct series provides us with a better picture on whether certain columns can still be salvaged with clever feature engineering such as appropriate Data Imputation. However, at a glance, we can already observed certain columns to be deemed unsuitable for further modelling or exploration:\n",
    "\n",
    "   \n",
    "    PoolQC – 99.5% missing\n",
    "    MiscFeature – 96.3% missing\n",
    "    Alley – 93.7% missing\n",
    "    Fence – 80.7% missing\n",
    "    FireplaceQu – 47% missing\n",
    "   \n",
    "    \n",
    "The above columns contains severe data quality issue with high % of missing values. This meant that even if we conduct data imputation, we may not be accuarately representing the data distribution of the particular feature, resulting in possible misinterpretation or skewing of the eventual models.\n",
    " \n",
    "The remaining columns are only having < 20% in missing values which is not a big issue. We can keep the columns for now, and further observe the columns during the EDA process to better make a judgement on whether to keep or drop the columns prior to modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab27241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the columns that deemed unsuitable for further modelling or exploration\n",
    "\n",
    "# create list of columns to drop\n",
    "drop_cols = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\n",
    "\n",
    "# drop the cols and save dataset into ./data/interim\n",
    "hse_data_interim = hse_data.drop(columns=drop_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9406b599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 76)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hse_data_interim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88c0c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hse_data_interim shape is 1460 rows and 76 columns.\n"
     ]
    }
   ],
   "source": [
    "# save as interim data\n",
    "hse_data_interim.to_csv(\"train_interim.csv\", index=False)\n",
    "\n",
    "print(\"The hse_data_interim shape is {} rows and {} columns.\".format(hse_data_interim.shape[0], hse_data_interim.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b575d",
   "metadata": {},
   "source": [
    "(Jensen) After running through a set of high-level data exploration, we understood that the train.csv – housing_data DataFrame, is not 100% perfect. We observed that there is a total of 23.46% columns (out of 81) highlighted to contain missing/null values. We, as a data analyst/scientist exploring this dataset should keep this in mind as we proceed further with Exploratory Data Analysis. We do not need to start cleaning any data yet but eventually, we will need find ways to prepare the housing_data DataFrame in a suitable way, in order for us to conduct appropriate machine-learning and generate certain predictive results.\n",
    "\n",
    "Hence as such, we have answered the following question in Section 0:\n",
    "\n",
    "    * What is the shape of your data i.e. number of rows and columns?\n",
    "    * For the numerical columns, what does the distributions look like?\n",
    "    * What is the name of the column to be predicted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583d0175",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738618f",
   "metadata": {},
   "source": [
    "**1. EDA**\n",
    "\n",
    "After we had spent some time digging through the dataset and answered a few of the questions on hand, we have now reached the Exploratory Data Analysis (EDA) stage, where we are going to utilise various visualisation methods such as Bar Chart, Pie Chart, Histograms, Scatter Plot and Line Plot to help visualise the data. After all, human are trained to retain visual information longer than written (textual) information.\n",
    "\n",
    "Before we begin, I hope that the course on data visualisation has provided you with the necessary skills to plot several different visualisations – where each often serves a specific purpose in communicating the insights in their own way.\n",
    "\n",
    "The following serves as a valuable cheatsheet if you are just starting out in Data Science. It provides you with a high level overview in what each visualisation attempts to communicate and the different types available.\n",
    "\n",
    "<!-- ![VisCheatsheet.jpg](../sup3-regression/home-data-for-ml-course/VisCheatsheet.jpg) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb515f8",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "<body>\n",
    "<img src=\"http://localhost:8888/view/ai4i/VisCheatsheet.jpg\" width=400 height=400 />\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047009c",
   "metadata": {},
   "source": [
    "The following questions will be answered in the following data visualization process.\n",
    "    * For the numerical columns, how many missing values are there for each column?\n",
    "    * For the categorical columns, how many missing values are there for each column?\n",
    "    * What visualizations can you use to highlight outliers in the data?\n",
    "\n",
    "Before we move on, remember 5 columns were dropped due to the high % of missing values present within the columns. \n",
    "Hence, the new csv saved in as train_interim.csv must be loaded for the exploratory process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d993d1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0   1          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1   2          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2   3          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "3   4          70       RL         60.0     9550   Pave      IR1         Lvl   \n",
       "4   5          60       RL         84.0    14260   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig  ... EnclosedPorch 3SsnPorch ScreenPorch PoolArea  \\\n",
       "0    AllPub    Inside  ...             0         0           0        0   \n",
       "1    AllPub       FR2  ...             0         0           0        0   \n",
       "2    AllPub    Inside  ...             0         0           0        0   \n",
       "3    AllPub    Corner  ...           272         0           0        0   \n",
       "4    AllPub       FR2  ...             0         0           0        0   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0       0      2    2008        WD         Normal     208500  \n",
       "1       0      5    2007        WD         Normal     181500  \n",
       "2       0      9    2008        WD         Normal     223500  \n",
       "3       0      2    2006        WD        Abnorml     140000  \n",
       "4       0     12    2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hse_data_interim = pd.read_csv(\"train_interim.csv\")\n",
    "# check dataframe\n",
    "hse_data_interim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361589cc",
   "metadata": {},
   "source": [
    "(Jensen) Thereafter, it will be useful for us to have two lists – consisting of the cols of each respective types, numerical and categorical. This will help us when I demonstrate how to automate the process of generating appropriate visualisations to ease the process of visualising ~70+ columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1fb0617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of categorical columns is 38.\n",
      "The number of numercial columns is 38.\n"
     ]
    }
   ],
   "source": [
    "# list non-numeric and numeric column names\n",
    "cat_cols = hse_data_interim.dtypes[hse_data.dtypes == 'object'].index.tolist()\n",
    "num_cols = hse_data_interim.dtypes[hse_data.dtypes != 'object'].index.tolist()\n",
    "\n",
    "\n",
    "print(\"The number of categorical columns is {}.\".format(len(cat_cols)))\n",
    "print(\"The number of numercial columns is {}.\".format(len(num_cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12297f91",
   "metadata": {},
   "source": [
    "(Jensen) Let's take a look at the numerical columns. One reasons to do a further inspection of the numerical columns is because, often than not, discrete values (e.g., 1,2,3,4 – which represents categories) are represented as numerical columns. Unlike continuous values – like SalePrice, trying to plot a chart like a Histogram on discrete may not provide as much insights as compared to something like a Boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb9685c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MSSubClass  OverallQual  OverallCond  BsmtFullBath  BsmtHalfBath  \\\n",
      "count  1460.000000  1460.000000  1460.000000   1460.000000   1460.000000   \n",
      "mean     56.897260     6.099315     5.575342      0.425342      0.057534   \n",
      "std      42.300571     1.382997     1.112799      0.518911      0.238753   \n",
      "min      20.000000     1.000000     1.000000      0.000000      0.000000   \n",
      "25%      20.000000     5.000000     5.000000      0.000000      0.000000   \n",
      "50%      50.000000     6.000000     5.000000      0.000000      0.000000   \n",
      "75%      70.000000     7.000000     6.000000      1.000000      0.000000   \n",
      "max     190.000000    10.000000     9.000000      3.000000      2.000000   \n",
      "\n",
      "          FullBath     HalfBath  BedroomAbvGr  KitchenAbvGr  TotRmsAbvGrd  \\\n",
      "count  1460.000000  1460.000000   1460.000000   1460.000000   1460.000000   \n",
      "mean      1.565068     0.382877      2.866438      1.046575      6.517808   \n",
      "std       0.550916     0.502885      0.815778      0.220338      1.625393   \n",
      "min       0.000000     0.000000      0.000000      0.000000      2.000000   \n",
      "25%       1.000000     0.000000      2.000000      1.000000      5.000000   \n",
      "50%       2.000000     0.000000      3.000000      1.000000      6.000000   \n",
      "75%       2.000000     1.000000      3.000000      1.000000      7.000000   \n",
      "max       3.000000     2.000000      8.000000      3.000000     14.000000   \n",
      "\n",
      "        Fireplaces   GarageCars     PoolArea       MoSold       YrSold  \n",
      "count  1460.000000  1460.000000  1460.000000  1460.000000  1460.000000  \n",
      "mean      0.613014     1.767123     2.758904     6.321918  2007.815753  \n",
      "std       0.644666     0.747315    40.177307     2.703626     1.328095  \n",
      "min       0.000000     0.000000     0.000000     1.000000  2006.000000  \n",
      "25%       0.000000     1.000000     0.000000     5.000000  2007.000000  \n",
      "50%       1.000000     2.000000     0.000000     6.000000  2008.000000  \n",
      "75%       1.000000     2.000000     0.000000     8.000000  2009.000000  \n",
      "max       3.000000     4.000000   738.000000    12.000000  2010.000000  \n"
     ]
    }
   ],
   "source": [
    "# Jensen's\n",
    "# hence, let's seperate the columns based on the description file found in '../../data/raw/data_description.txt'\n",
    "\n",
    "uniq_threshold = 20  # arbitrary number\n",
    "discrete_num_cols = [col for col in hse_data_interim.columns if (hse_data_interim[col].nunique() < uniq_threshold) & (hse_data_interim[col].dtype != 'object') ]\n",
    "\n",
    "# saving the continuous cols in a seperate list for ease of usage\n",
    "cont_num_cols = [col for col in num_cols if col not in discrete_num_cols]\n",
    "\n",
    "# running a small check; total of 15 discrete columns\n",
    "# print(hse_data_interim.loc[:,discrete_num_cols])\n",
    "# printing summmary \n",
    "print(hse_data_interim.loc[:,discrete_num_cols].describe())\n",
    "\n",
    "# sanity check to make sure len(disc_num_cols) + (cont_num_cols) == len(num_cols)\n",
    "\n",
    "assert(len(discrete_num_cols) + len(cont_num_cols) == len(num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5b153",
   "metadata": {},
   "source": [
    "(Jensen) **1.1 Visualising Distributions in Numerical Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe89bad",
   "metadata": {},
   "source": [
    "So, how are we going to visualize numerical columns? One of the questions we aim to answer through EDA, is how are the numerical columns distributed? Some first thoughts that come to mind are Histograms, Scatter Plot. And guess, what? Seaborn provides just the right visualisation that we need!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62610dbe",
   "metadata": {},
   "source": [
    "As we explain previously on why we are seperating between discrete numerical vs continuous numerical data, in this portion, we will be plotting for continuous numerical data first in order to see the distribution between the features vs the target (a bit on the correlation here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74760656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['Id', 'LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'SalePrice']\n"
     ]
    }
   ],
   "source": [
    "print(len(cont_num_cols))\n",
    "print(cont_num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jensen's\n",
    "# to prevent cramped visualisations, we will seperate and plot in groups of 4 + 1 (target) \n",
    "# to better understand both the distribution of the numerical column and \n",
    "# also the relationship (scatterplot) between the numerical columns and the target.\n",
    "\n",
    "# temp removing the target \n",
    "cont_num_cols.remove('SalePrice')\n",
    "# check to ensure num of cont cols as expected\n",
    "assert(len(cont_num_cols) == 22)\n",
    "\n",
    "for i in range(0, len(cont_num_cols), 4):\n",
    "    # plotting 4 + 1 (target)\n",
    "    curr_cols = cont_num_cols[i:i+4] + ['SalePrice']\n",
    "    sns.pairplot(hse_data_interim.loc[:,curr_cols], diag_kind='kde', size=2, dropna=True)\n",
    "    \n",
    "    # saving figure\n",
    "    # plt.savefig(f\"../reports/figures/pairplot_{i}_{i+4}.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508388e",
   "metadata": {},
   "source": [
    "After generating respective pairplot of continuous cols (features) against SalePrice, let's do a quick overview analysis on some patterns that we can spot and should deep-dive further in subsequent EDA procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a029befa",
   "metadata": {},
   "source": [
    "Looking at the Scatterplot and Histogram, the first thing that comes to mind is that majority of the continuous numerical data are not of Normal Distributions. This may pose a potential problem if machine learning algorithms that assume normally distributed data. However, we do have other algorithms that is non-parametric in nature – meaning that no prior assumption of the data distribution is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3ff27",
   "metadata": {},
   "source": [
    "Besides looking at the distribution, we should also focus on looking out of potential correlated features with either: **1) the target – SalePrice or 2) other features**. So you may ask do we need to look out of either correlations?\n",
    "\n",
    "It is because:\n",
    "\n",
    "**If a feature is correlated with the target, it may provide some insights on how that particular features may affect the final prediction.** For instance, looking at GrLivingArea and SalePrice, we can see that the feature is positively correlated with the target. So what exactly is GrLivingArea? Based on the data description, we understand that the feature refers to Above grade (ground) living area square feet – which may be simplfied as how many more extra living area is available above the ground floor, which logically suggests the relationship where, the more living area, the higher the potential price.\n",
    "\n",
    "The above provides an analytical though process on how you should leverage the visualisations to help you better understand your data. Subsequently, we will leverage on these existing information, to conduct several more in-depth statistical tests (Pearson's Correlation, Spearman's Correlation) and visualisation (Correlation Plot). This enables us to make wiser decisions on which additional columns should be removed, engineered and keep for the final modelling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a79381",
   "metadata": {},
   "source": [
    "Alright! After looking at the continous numerical data, we should now start exploring the discrete continuous data instead. As mentioned previously, discrete numerical data may be in a numerical data in nature, but rather they perform similarly like a categorical features. Hence, Scatterplot may not be the best visualisation for us to retrieve valuable insights. Instead, we will be leveraging on Boxplot to help see the Interquartile Range and also highlight any potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function def for plotting distributions of discrete cols using box vs bar \n",
    "def plot_cat_distribution(x, y, data):\n",
    "    # plt.subplots(nrows, ncols, figsize=(x, y), sharey=True:y-axis will be shared among all subplots.)\n",
    "    # (ax1, ax2) using tuple unpacking for multiple Axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "\n",
    "    # plotting bar plot vs box plot\n",
    "    sns.barplot(x, y, data=data, ax=ax1, palette='rocket', ci=False)\n",
    "    sns.boxplot(x, y, data=data, ax=ax2, palette='rocket')\n",
    "\n",
    "    # giving title\n",
    "    fig.suptitle(f'Distribution of {x}')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting boxplot for each discrete columns against the sale price\n",
    "\n",
    "for col in discrete_num_cols:\n",
    "    plot_cat_distribution(col, 'SalePrice', data=hse_data_interim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35bb56",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title></title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<p>\n",
    "    Looking at the boxplots, don't you find it easy to spot potential outliers? That is one observation, I have on my end. Looking through all the discrete features, we observed that several (or majority) of the features consists of potential outliers. Why do I say potential? Because such outliers may still be a valuable information that is part of our dataset. In essence, if you cannot justify why you would want to remove the outliers – you are basically conforming your model to assume less variability that is actually occuring in the actual \"real\" world dataset. Hence, a <a href=\"https://statisticsbyjim.com/basics/remove-outliers/\">guideline</a>  which I find useful would be the following, in determine outliers:\n",
    "   \n",
    "   \n",
    "</p>\n",
    "    <ol>\n",
    "      <li>A measurement error or data entry error, correct the error if possible. If you can't fix it, remove the observation because you know it's incorrect.</li>\n",
    "      <li>Not a part of the population you are studying (i.e., unusual properties or conditions), you can legitimately remove the outlier.\n",
    "      </li>\n",
    "      <li>If the data point is a natural part of the population you are studying, you should NOT remove it.\n",
    "      </li>\n",
    "    </ol>\n",
    "   \n",
    "    \n",
    "<p> \n",
    "        However, another approach – through experimental design is to run the model through two different datasets: 1) one with outliers removed, 2) one without outliers removed. These may be useful to validate your doubts on whether to remove the outliers after actually seeing the validation metrics.\n",
    "</p>\n",
    "    \n",
    "<p> \n",
    "        Hence, for now, we will still leave the outliers in, but let's still run a bunch of summary statistics to better determine if the outliers are actual errors as opposed to just naturally occuring outliers.\n",
    "</p> \n",
    "    \n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3a1dbb",
   "metadata": {},
   "source": [
    "**1.1.1 Visualising Correlations in Numerical Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ff210",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title></title>\n",
    "</head>\n",
    "<body>\n",
    "    <div>\n",
    "    <p> \n",
    "    After plotting a few charts in an attempt to understand the underlying distributions within the numerical columns, we have another question. We have tried to explain briefly earlier on, how through the pairplots, we are able to observe both the distributions as well as correlations through the scatterplots. However, as we have quite a lot of continuous numerical columns, we are unable to see the bigger pictures and identify even more potentially correlated features.\n",
    "    </p> \n",
    "    <p>\n",
    "       The reason why these correlated features matters, is that correlated features in general <a href=\"https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features\">do not improve models</a>. Let's put things into perspective, given that we asked a user about the number of child he/she has, and we also asked he/she the number of son & daughters seperately. This presents in a correlation. Why? The number of child is positively correlated with number of son and number of daughter. If an user say that she has 3 daughters, the number of child would likely be 3 or more. Hence, a positive increment in both <code>sons</code> and <code>daughters</code> will result in an increase in <code>child</code> as well.\n",
    "    </p>\n",
    "    <p>\n",
    "            In Machine Learning, there's a principle of parsimony, or <a href=\"https://en.wikipedia.org/wiki/Occam%27s_razor\">Occam's razor</a>, where a simpler model is preferred as opposed to building a model with 100s or 1000s or features, especially when a simple model can provide similar or even better results (potential overfitting if you have so many features, resulting in poor performance).\n",
    "    </p>\n",
    "    <p>\n",
    "            Hence, let's leverage on Seaborn's heatmap plots to better help us visually identify such correlated features, which we will then go through the data description to see why they might be correlated.\n",
    "    </p>\n",
    "    </div>   \n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12308593",
   "metadata": {},
   "source": [
    "# finding plot a better visualization to better observe which features \n",
    "# correlates the most with `SalePrice`\n",
    "\n",
    "def plot_corr_map(cont_cols, figsize=(25, 20)):\n",
    "    # references: https://medium.com/analytics-vidhya/kaggle-house-prices-prediction-with-linear-regression-and-gradient-boosting-c5694d9c6df4\n",
    "    # subset data\n",
    "    cont_data = hse_data_interim.loc[:, cont_num_cols + ['SalePrice']] \n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(12, 9))    \n",
    "    sns.heatmap(cont_data, vmax=0.8, square=True, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eea185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jensen's\n",
    "# plotting correlation heatmap\n",
    "\n",
    "def plot_corr_map(cont_cols, figsize=(25,20)):\n",
    "    # references: https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n",
    "\n",
    "    # dataframe subset\n",
    "    cont_data = hse_data_interim.loc[:, cont_cols+['SalePrice']]\n",
    "\n",
    "    # define mask to isolate upper right triangle\n",
    "    mask = np.triu(np.ones_like(cont_data.corr(), dtype=np.bool))\n",
    "\n",
    "    # setting plot size\n",
    "    plt.figure(figsize=figsize)\n",
    "    heatmap = sns.heatmap(cont_data.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612a07e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_corr_map(cont_num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c89d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding plot a better visualization to better observe which features correlates the most with `SalePrice`\n",
    "\n",
    "def plot_correlation_bar(cont_cols, figsize=(10, 15)):\n",
    "    \n",
    "    # subset data\n",
    "    corr_sales = hse_data_interim.loc[:, cont_num_cols + ['SalePrice']].corr()[['SalePrice']]\n",
    "\n",
    "    # setting plotting figure\n",
    "    plt.figure(figsize=figsize)\n",
    "    heatmap = sns.heatmap(corr_sales.sort_values(by='SalePrice', ascending=False), vmin=-1, vmax=1, annot=True, cmap='coolwarm')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdf4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_bar(cont_num_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1790ab",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "</head>\n",
    "    <body>\n",
    "        <p>\n",
    "            Looking at the correlaion heatmap, we can sieve through some columns that we possibly drop in the later part of the project during the <code>Feature Selection</code> stage. Here's are some of the columns and the justifications on why these columns are highlighted:\n",
    "        </p> \n",
    "\n",
    "            <ol>\n",
    "                 <li><em>GarageYrBlt</em> – it has correlation coefficient of <code>0.83</code> with <em>YearBuilt</em>, which is logical considering that the garage either comes along with the house or built later on. Usually one would tend to observe which year the house was constructed as oppose to the garage as the garage is just small extension of the house.\n",
    "                  </li>\n",
    "                  <li><em>TotalBsmtSF</em> – it has a correlation coefficient of <code>0.82</code> with <em>1stFlrSF</em>, which is probable considering basement under the first floor of the house tends to be similarly sized in square feet or smaller.\n",
    "                  </li>\n",
    "            </ol>\n",
    "          \n",
    "          \n",
    "     \n",
    "        <p>Thus far we set our threshold at <code>>0.70</code>, as besides manually selecting features to drop, we can also depend on other feature selections method that we will be discussing in the later part of the project.<br>\n",
    "            Lastly, if you have not noticed, there are other column(s) that need to be dropped not because of possible collinearity but rather the columns provides no values to the modelling later. Take a look at <code>Id</code>, it is a column to identify each record but does not mean anything to the <code>SalePrice</code>.\n",
    "        </p>\n",
    "        <p> \n",
    "    Hence, based on this section of the exploration, we have identified TotalBsmBlt, GarageYrBlt and Id that would be removed at the end of this section before further preprocessing.\n",
    "\n",
    "        </p>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002624f",
   "metadata": {},
   "source": [
    "**1.2 Visualising Distributions in Categorical Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564331b6",
   "metadata": {},
   "source": [
    "\n",
    "After attempting to plot some visualisations to understand the distributions of the numerical columns, we now turn our eyes to plotting the categorical columns. Plotting categorical columns is just similar to plotting discrete numerical columns as both have discrete values. For instance <code>0/1</code> in discrete numerical column can also be represented as <code>Yes/No</code> in a categorical column.\n",
    "\n",
    "Beside plotting a Bar Plot, we can also use a Box Plot similar to what we did for the discrete numerical columns. This enables us to observe the 25th, 50th (median), 75th percentile of the distribution in the <code>SalePrice</code>.\n",
    "\n",
    "So, let's first plot some bar and box plot to help understand the value counts within each columns and identify if there's any values of any columns that is either in overwhelming majority or underwhelming minority.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc66940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    plot_cat_distribution(col, 'SalePrice', data=hse_data_interim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a883419",
   "metadata": {},
   "source": [
    "As an overview, the distribution of the categorical columns provides us a good look at how the each category represents themselves in terms of both the *average* <code>SalePrice</code> and also the range of <code>SalePrice</code> it encompasses. Similarly to what we observed for the *discrete numerical* columns, we still observe possible outliers here and there, but as we are cannot be completely sure that outliers occurs non-naturally (e.g., human errors), we shall not remove these information. Still, we can conduct possible experiment subsequently to compare the results between removal and non-removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d36c9",
   "metadata": {},
   "source": [
    "**1.2.1 Visualising Correlations in Categorical Columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3334604",
   "metadata": {},
   "source": [
    "Many of you may have understood the concept of Pearson R coefficient used in determining correlation between two continous fields. However, we can also use something known as <a href=\"https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\">Cramer's V</a> to help us to understand the relationship between two categorical fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317dd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROC, AUC,\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(figsize=(25,25))\n",
    "associations(hse_data_interim.loc[:,cat_cols], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd314d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_correlations = hse_data_interim.corr()\n",
    "top_feature_columns = top_correlations['SalePrice'][top_correlations['SalePrice'].values > 0.2].index.values\n",
    "top_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36df6a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# references: https://medium.com/analytics-vidhya/kaggle-house-prices-prediction-with-linear-regression-and-gradient-boosting-c5694d9c6df4\n",
    "\n",
    "# Handling Missing Values for 19 features which have missing values mentioned above\n",
    "hse_data_interim['GarageYrBlt'] = hse_data_interim['GarageYrBlt'].fillna(0)\n",
    "# filling in missing GarageYrBuilt values with zeros.  \n",
    "# But this may not be the most logical approach - refer to this discussion below for mor perspective\n",
    "# https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/60143\n",
    "\n",
    "# similary fillingup na valuse for couple of other features\n",
    "hse_data_interim['LotFrontage'] = hse_data_interim.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "hse_data_interim['MasVnrArea'] = hse_data_interim['MasVnrArea'].fillna(0)\n",
    "heat_map_with_top_correlated_features = np.append(top_feature_columns[-12:], np.array(['SalePrice']))\n",
    "pearson_correlation_coefficients = np.corrcoef(hse_data_interim[heat_map_with_top_correlated_features[::-1]].T)\n",
    "plt.figure(figsize=(16,16))\n",
    "sns.set(font_scale=1)\n",
    "with sns.axes_style('white'):\n",
    "    sns.heatmap(pearson_correlation_coefficients, yticklabels=heat_map_with_top_correlated_features[::-1], xticklabels=heat_map_with_top_correlated_features[::-1], fmt='.2f', annot_kws={'size': 10}, annot=True, square=True, cmap=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9589de",
   "metadata": {},
   "source": [
    "Now lets glance over the above heatmap to understand some points just visualy looking at the correlation number\n",
    "\n",
    "‘OverallQual’, ‘GrLivArea’ and ‘TotalBsmtSF’ have strong correlation with ‘SalePrice’.\n",
    "\n",
    "While ‘GarageCars’ and ‘GarageArea’ also have strong correlation, but they are mostly mutually dependent i.e. they are NOT linearly independent of each other, i.e. there is a high-multicollinearity (0.88 as we can see in the figure) between them. Because, the number of cars that fit into the garage is dependent of the garage area. Hence, we just need one of these variables in our analysis (we can decide to keep ‘GarageCars’ as its correlation with ‘SalePrice’ is higher).\n",
    "\n",
    "Same mutual dependence applies to the two features ‘TotalBsmtSF’ and ‘1stFloor’ . We will take only ‘TotalBsmtSF’ in our feature-engineering.\n",
    "\n",
    "AND ALSO ‘TotRmsAbvGrd’ and ‘GrLivArea’, and we will only take ‘GrLivArea’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057fc60b",
   "metadata": {},
   "source": [
    "# references: Jensen's: https://nbviewer.jupyter.org/github/wtlow003/aiig-suss/blob/main/learning_materials/regression/notebooks/01-lwt-initial-exploratory-data-analysis.ipynb\n",
    "\n",
    "Likewise to using Pearson's R for correlations between continuous features, we can also use Cramer's V and Theil's U in a similar fashion. The heatmap that we have plotted above is symmetrical in nature - which is one of the properties of Cramer's V. It comes with certain pitfalls, but in this first EDA, we will not discuss about it. Here are some features that we can possibly drop:\n",
    "\n",
    "1. *GarageQual* – it has a value of <code>0.7</code> with *GarageCond*, which higlights a high association between the two nominal features. This association makes sense considering that both *GarageQual* and GarageCond are trying to quantify the garage in a measurable way. Hence, we can just consider keeping 1 of the columns instead.\n",
    "\n",
    "2. *Exterior2nd* – it has a value of <code>0.76</code> with *Exterior1st*, which highlights a high assocation between the two nominal features. Considering that both describes the exterior covering on the house, where Exterior2nd is applicable, if more than one material is used. Instead of dropping this feature, we can consider conducting <code>Feature Engineering</code> and generate another column on how many exterior finishing is used, while we still use the main exterior finishing – Exterior1st as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ae5593",
   "metadata": {},
   "source": [
    "**2. Concluding Remarks**\n",
    "\n",
    "So after plotting and spending some time looking through the plot and generating relevant hypothesis on which features we can possible drop to reduce the features required for the model, and also answer some questions we had on hand prior to conducting EDA. To bring ourselves back to the current process of the data preprocesssing – we have 4 columns to remove. They are:\n",
    "\n",
    "1. <code>GarageYrBlt</code>\n",
    "2. <code>TotalBsmtSF</code>\n",
    "3. <code>Id</code>\n",
    "4. <code>GarageQual</code>\n",
    "\n",
    "We will be saving this DataFrame in its interim stage while we proceed on with further <code>Feature Selection & Feature Engineering</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop\n",
    "drop_cols = ['GarageYrBlt', 'TotalBsmtSF', 'Id', 'GarageQual']\n",
    "\n",
    "hse_data_interim = hse_data_interim.drop(drop_cols, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dfab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hse_data_interim.to_csv(\"train_csv_EDA.csv\", index=False) \n",
    "\n",
    "# check the current updated dataframe shape to ensure only 4 columns are dropped\n",
    "print(f\"The current DataFrame shape is: {hse_data_interim.shape[0]} rows, {hse_data_interim.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc82efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ca28155",
   "metadata": {},
   "source": [
    "**--------------My own random Viz------------------**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "Distribution for numerical features\n",
    "- many data points\n",
    "- scatter plot \n",
    "- box plot for summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25472430",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='YearBuilt', y='SalePrice', data=df, hue='SaleCondition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: plotting as FacetGrid: can create subplots\n",
    "sns.set_context('notebook')\n",
    "sns.set_palette('RdBu')\n",
    "sns.set_style('darkgrid')\n",
    "g = sns.catplot(x='YearBuilt', y='SalePrice', data=df, kind='box', col=\"SaleCondition\")\n",
    "\n",
    "# give a title and position the title\n",
    "g.fig.suptitle(\"Year Built vs. Sale Price\", y=1.03)\n",
    "\n",
    "# give titles for each column\n",
    "g.set_titles(\"this is column name: {col_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: plotting as AxesSubplots: only creates single plot; scatterplot(), countplot(), etc.\n",
    "sns.set_context('paper')\n",
    "sns.set_palette('RdBu')\n",
    "sns.set_style('darkgrid')\n",
    "g = sns.boxplot(x='YearBuilt', y='SalePrice', data=df)\n",
    "\n",
    "# give a title and position the title\n",
    "g.set_title(\"Year Built vs. Sale Price\", y=1.03)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee88825",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target variable (column to be predicted)  --> SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2dcddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "How are the various attributes correlated to the outcome variable?\n",
    "yearBuilt, lotArea, miscval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704b959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be20b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the numerical columns, how many missing values are there for each column? \n",
    "# find missing values for each column\n",
    "missing_val = df.isnull().sum()\n",
    "print(missing_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35579695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['MiscVal'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9929ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['MiscFeature'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302088f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "For the categorical columns, how many missing values are there for each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize to highlight the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1944d39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595cff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e20e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_feats = df.select_dtypes(include=['float64', 'int64'])\n",
    "numeric_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df314a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8338fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_feats = df.select_dtypes(include=['object'])\n",
    "object_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['YearBuilt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e19e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
